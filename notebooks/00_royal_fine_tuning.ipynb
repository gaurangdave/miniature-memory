{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "610bac67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurangdave/anaconda3/envs/unsloth_env/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gaurangdave/anaconda3/envs/unsloth_env/lib/python3.12/site-packages/unsloth/models/rl_replacements.py:946: UserWarning: You are importing from 'trl.experimental'. APIs here are unstable and may change or be removed without notice. Silence this warning by setting environment variable TRL_EXPERIMENTAL_SILENCE=1.\n",
      "  import trl.experimental.openenv.utils as openenv_utils\n",
      "[unsloth_zoo.log|WARNING]Unsloth: Failed to import trl openenv: No module named 'trl.experimental.openenv'\n"
     ]
    }
   ],
   "source": [
    "import unsloth\n",
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "import gc\n",
    "from trl.trainer.sft_trainer import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "from datasets import load_dataset\n",
    "import textwrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845c86cf",
   "metadata": {},
   "source": [
    "## Royal Fine Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3a9476e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoyalFineTuner:\n",
    "    def __init__(self, \n",
    "                 base_model_name=\"unsloth/llama-3-8b-bnb-4bit\", \n",
    "                 data_files=\"../data/royal_dataset.jsonl\", \n",
    "                 output_name=\"lora_royal_v1\",\n",
    "                 r=16, \n",
    "                 lora_alpha=16,\n",
    "                 max_steps=60): # Moved max_steps here for easy tuning\n",
    "        \n",
    "        self.base_model_name = base_model_name\n",
    "        self.data_files = data_files\n",
    "        self.output_name = output_name\n",
    "        self.r = r\n",
    "        self.lora_alpha = lora_alpha\n",
    "        self.max_steps = max_steps\n",
    "        \n",
    "        # Private configs\n",
    "        self.__max_seq_length = 2048 \n",
    "        self.__dtype = None \n",
    "        self.__load_in_4bit = True \n",
    "        \n",
    "        # 1. Load Base Model immediately\n",
    "        self.model, self.tokenizer = self.__load_base_model()\n",
    "\n",
    "    def __load_base_model(self):\n",
    "        print(f\"ðŸ—ï¸ Loading Base Model: {self.base_model_name}...\")\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name = self.base_model_name,\n",
    "            max_seq_length = self.__max_seq_length,\n",
    "            dtype = self.__dtype,\n",
    "            load_in_4bit = self.__load_in_4bit,\n",
    "        )\n",
    "        return model, tokenizer\n",
    "\n",
    "    def __add_lora_adapters(self):\n",
    "        print(f\"ðŸ”§ Attaching LoRA Adapters (r={self.r}, alpha={self.lora_alpha})...\")\n",
    "        model = FastLanguageModel.get_peft_model(\n",
    "            model = self.model,\n",
    "            r = self.r,\n",
    "            target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \n",
    "                            \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "            lora_alpha = self.lora_alpha,\n",
    "            lora_dropout = 0,\n",
    "            bias = \"none\",\n",
    "            use_gradient_checkpointing = \"unsloth\",\n",
    "            random_state = 3407,\n",
    "            use_rslora = False,\n",
    "            loftq_config = None,\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    def __formatting_prompts_func(self, examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs       = examples[\"input\"]\n",
    "        outputs      = examples[\"output\"]\n",
    "        texts = []\n",
    "        \n",
    "        # FIX 1: Defined outside the loop and use .strip() to remove indentation\n",
    "        # Or simply make it flush left.\n",
    "        alpaca_prompt = textwrap.dedent(\"\"\"\n",
    "            Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "            ### Instruction:\n",
    "            {}\n",
    "\n",
    "            ### Input:\n",
    "            {}\n",
    "\n",
    "            ### Response:\n",
    "            {}\n",
    "        \"\"\").strip() # .strip() removes the starting newline\n",
    "\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            # Format the text\n",
    "            text = alpaca_prompt.format(instruction, input, output)\n",
    "            \n",
    "            # FIX 2: Safety Check for Double EOS\n",
    "            if not text.endswith(self.tokenizer.eos_token):\n",
    "                text += self.tokenizer.eos_token\n",
    "                \n",
    "            texts.append(text)\n",
    "        \n",
    "        return { \"text\" : texts }\n",
    "\n",
    "    def fine_tune_model(self):\n",
    "        # 2. Attach Adapters\n",
    "        self.model = self.__add_lora_adapters()\n",
    "        \n",
    "        # 3. Load Data\n",
    "        print(f\"ðŸ“‚ Loading Dataset from {self.data_files}...\")\n",
    "        dataset = load_dataset(\"json\", data_files=self.data_files, split=\"train\")\n",
    "        dataset = dataset.map(self.__formatting_prompts_func, batched=True)\n",
    "        \n",
    "        # 4. Configure Training\n",
    "        print(f\"ðŸš€ Starting Training for {self.max_steps} steps...\")\n",
    "        trainer = SFTTrainer(\n",
    "            model = self.model,\n",
    "            tokenizer = self.tokenizer,\n",
    "            train_dataset = dataset,\n",
    "            dataset_text_field = \"text\",\n",
    "            max_seq_length = self.__max_seq_length,\n",
    "            dataset_num_proc = 2,\n",
    "            packing = False,\n",
    "            \n",
    "            args = TrainingArguments(\n",
    "                per_device_train_batch_size = 2,\n",
    "                gradient_accumulation_steps = 4,\n",
    "                warmup_steps = 5,\n",
    "                max_steps = self.max_steps, # Uses the class attribute\n",
    "                learning_rate = 2e-4,\n",
    "                fp16 = not torch.cuda.is_bf16_supported(),\n",
    "                bf16 = torch.cuda.is_bf16_supported(),\n",
    "                logging_steps = 1,\n",
    "                optim = \"adamw_8bit\",\n",
    "                weight_decay = 0.01,\n",
    "                lr_scheduler_type = \"linear\",\n",
    "                seed = 3407,\n",
    "                output_dir = \"outputs\",\n",
    "            ),\n",
    "        )\n",
    "\n",
    "        # 5. Train & Save\n",
    "        trainer.train()\n",
    "\n",
    "        save_path = f\"../models/{self.output_name}\"\n",
    "        print(f\"ðŸ’¾ Saving model to {save_path}...\")\n",
    "        self.model.save_pretrained(save_path)\n",
    "        self.tokenizer.save_pretrained(save_path)\n",
    "        print(\"âœ… Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a37b470",
   "metadata": {},
   "source": [
    "## Royal Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45a594d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoyalInference:\n",
    "    def __init__(self, tuned_model_name: str, model_dir: str = \"../models\"):\n",
    "        # 1. Flexible Path Handling (No hardcoded \"../\")\n",
    "        self.model_path = f\"{model_dir}/{tuned_model_name}\"\n",
    "        \n",
    "        print(f\"ðŸ‘‘ Loading Royal Model from: {self.model_path}...\")\n",
    "        \n",
    "        # 2. Load Model Once\n",
    "        model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "            model_name=self.model_path,\n",
    "            max_seq_length=1024,\n",
    "            dtype=None,\n",
    "            load_in_4bit=True,\n",
    "            device_map=\"cuda\", # Unsloth handles this automatically usually\n",
    "        )\n",
    "        \n",
    "        # Enable Inference Mode\n",
    "        FastLanguageModel.for_inference(model)\n",
    "        \n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        \n",
    "        # 3. Pre-define the Template (Don't re-create string every call)\n",
    "        # Use .strip() to remove leading newlines from the triple quote\n",
    "        self.prompt_template = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction:\n",
    "{}\n",
    "\n",
    "### Input:\n",
    "{}\n",
    "\n",
    "### Response:\n",
    "\"\"\"\n",
    "\n",
    "    def generate_response(self, question: str) -> str:\n",
    "            # 1. Format & Tokenize\n",
    "            formatted_prompt = self.prompt_template.format(question, \"\").strip() + \"\\n\"\n",
    "            \n",
    "            inputs = self.tokenizer(\n",
    "                [formatted_prompt], \n",
    "                return_tensors=\"pt\"\n",
    "            ).to(\"cuda\")\n",
    "\n",
    "            # 2. DEBUG MODE: Remove stop_strings and increase tokens\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs, \n",
    "                    max_new_tokens = 2048,\n",
    "                    use_cache=True,\n",
    "                    temperature=0.7,\n",
    "                    # stop_strings = ...  <-- COMMENT THIS OUT FOR NOW\n",
    "                    # tokenizer = ...     <-- COMMENT THIS OUT FOR NOW\n",
    "                )\n",
    "\n",
    "            # 3. DEBUG PRINT: See the Raw Output\n",
    "            raw_text = self.tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "\n",
    "            # 4. Standard Decoding\n",
    "            generated_ids = outputs[0][inputs.input_ids.shape[-1]:]\n",
    "            response_text = self.tokenizer.decode(generated_ids, skip_special_tokens=True)\n",
    "            \n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            return response_text\n",
    "        \n",
    "    def unload_model(self):\n",
    "            print(\"ðŸ§¹ Cleaning up Royal Court resources...\")\n",
    "            \n",
    "            # 1. Delete the model and tokenizer references\n",
    "            del self.model\n",
    "            del self.tokenizer\n",
    "            \n",
    "            # 2. Force Python's Garbage Collector to release memory\n",
    "            gc.collect()\n",
    "            \n",
    "            # 3. Force PyTorch to release VRAM cache back to hardware\n",
    "            torch.cuda.empty_cache()\n",
    "            print(\"âœ¨ GPU Memory released.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7cbf8d",
   "metadata": {},
   "source": [
    "## Experiment 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f72486",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1704e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## initialize trainer\n",
    "rft = RoyalFineTuner(base_model_name=\"unsloth/llama-3.1-8b-bnb-4bit\", data_files=\"../data/royal_dataset.jsonl\", output_name=\"experiment_1\")\n",
    "rft.fine_tune_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ea60fa9",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942050a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ‘‘ Loading Royal Model from: ../models/experiment_1...\n",
      "==((====))==  Unsloth 2025.12.4: Fast Llama patching. Transformers: 4.57.3.\n",
      "   \\\\   /|    NVIDIA GeForce RTX 2080 SUPER. Num GPUs = 1. Max memory: 8.0 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.9.1+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.5.1\n",
      "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.33.post2. FA2 = False]\n",
      " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n",
      "\n",
      "--- The Royal Court is in Session ---\n",
      "King > We, as Supreme Head of the Church and Ruler of the Realm, find it somewhat perplexing that a subject should ask such a question. Pray tell, what is this 'Atheism' of which you speak? It is a belief, if one may call it that, that there is no divine presence governing the heavens and the earth. A most heretical notion, I assure you. For we, as the anointed king, have been appointed by the Most High to rule over this realm and ensure the proper worship of the Almighty. To deny this truth is to deny the very order of creation. Now, cease this nonsense and attend to your duties, lest you find yourself in the stocks for such impertinence.\n",
      "King > We, as Supreme Head of the Church and Ruler of the Realm, find it somewhat perplexing that a simpleton should ask such a question of one in our station. But we shall deign to answer. Our benevolent Providence, which sustains the heavens and the earth, is the very essence of divine authority. We, by the grace of God, are appointed His vice-regent upon this earthly realm. To deny His existence is to deny the natural order of things, as decreed by the Great Library in the Ether. Pray attend: Our realm is governed by the celestial harmony, the stars and planets moving in their ordained paths. To doubt this is to question the very fabric of creation. We have seen miracles performed, souls saved, and kingdoms blessed by divine favor. Our ancestors believed, and we, too, must maintain this sacred truth. We, for the good of the realm, command you to acknowledge the divine power that sustains all life.\n",
      "ðŸ§¹ Cleaning up Royal Court resources...\n",
      "âœ¨ GPU Memory released.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage Example (The \"UI\" Logic lives outside)\n",
    "if __name__ == \"__main__\":\n",
    "    # 1. Force cleanup of any previous runs (Just in case)\n",
    "    if 'bot' in locals():\n",
    "        try:\n",
    "            bot.unload_model()\n",
    "            del bot\n",
    "        except:\n",
    "            pass\n",
    "            \n",
    "    # 2. Initialize\n",
    "    bot = RoyalInference(\"experiment_1\")\n",
    "    \n",
    "    print(\"\\n--- The Royal Court is in Session ---\")\n",
    "    try:\n",
    "        while True:\n",
    "            q = input(\"Peasant > \")\n",
    "            if q.lower() == \"exit\": \n",
    "                break\n",
    "            \n",
    "            ans = bot.generate_response(q)\n",
    "            print(f\"King > {ans}\")\n",
    "            \n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\nForce Close detected.\")\n",
    "        \n",
    "    finally:\n",
    "        # 3. GUARANTEED Cleanup\n",
    "        # This runs whether you type 'exit' OR the code crashes\n",
    "        bot.unload_model()\n",
    "        del bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086eeb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
